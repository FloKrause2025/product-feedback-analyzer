{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ebd48d7",
   "metadata": {},
   "source": [
    "## Product Analyzer Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe97bb",
   "metadata": {},
   "source": [
    "# Section 1: Setup & Environment\n",
    "\n",
    "In this section, we will install the necessary libraries and set up our credentials to connect to the Reddit and Product Hunt APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9af241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (7.8.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.32.4)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2025.7.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/bin/python3.11 -m pip install praw requests python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855cf300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Reddit credentials\n",
    "reddit_client_id = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "reddit_client_secret = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "\n",
    "# Get the Product Hunt credentials\n",
    "ph_api_key = os.getenv(\"PRODUCT_HUNT_API_KEY\")\n",
    "ph_access_token = os.getenv(\"PRODUCT_HUNT_ACCESS_TOKEN\")\n",
    "\n",
    "# Check if the keys were loaded correctly\n",
    "if reddit_client_id and ph_api_key:\n",
    "    print(\"‚úÖ API keys loaded successfully!\")\n",
    "else:\n",
    "    print(\"üö® Error: Could not load API keys. Check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36a52c",
   "metadata": {},
   "source": [
    "# Data collection Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef87abf",
   "metadata": {},
   "source": [
    "# Connection to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df2c16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Reddit as: None (Read-Only Mode: True)\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "# Use the loaded keys to initialize the Reddit API connection\n",
    "# A user_agent is a unique name for your script so Reddit knows who is making requests.\n",
    "try:\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=reddit_client_id,\n",
    "        client_secret=reddit_client_secret,\n",
    "        user_agent=\"Feature-Analyzer by Flo\"\n",
    "    )\n",
    "\n",
    "    # Check the connection status\n",
    "    print(f\"‚úÖ Connected to Reddit as: {reddit.user.me()} (Read-Only Mode: {reddit.read_only})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"üö® Error connecting to Reddit: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c8374",
   "metadata": {},
   "source": [
    "# Fetching Data from your Subreddit - in our case \"n8n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5110c909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Fetching the top 10 hot posts from r/n8n...\n",
      "  - [Score: 2] Weekly Self Promotion Thread\n",
      "  - [Score: 28] How much is my workflow worth?\n",
      "  - [Score: 15] We'll never know\n",
      "  - [Score: 61] Anyone here looking for a job?\n",
      "  - [Score: 14] Hiring n8n Expert (Bonus if Full Stack w/ Node.js + React)\n",
      "  - [Score: 25] GitHub Releases to Marketing\n",
      "  - [Score: 4] Things I have automated, do you want yours?\n",
      "  - [Score: 2] Self hosting via Docker security risk?\n",
      "  - [Score: 5] How I Use Redis to Cache Google API Data in n8n (and Why You Should Too)\n",
      "  - [Score: 3] I want to create a did you mean loop\n"
     ]
    }
   ],
   "source": [
    "# Define the target subreddit and how many posts to fetch\n",
    "subreddit_name = \"n8n\"\n",
    "post_limit = 10\n",
    "\n",
    "print(f\"üî• Fetching the top {post_limit} hot posts from r/{subreddit_name}...\")\n",
    "\n",
    "try:\n",
    "    # Point our connection to the target subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Fetch the 'hot' posts and loop through them\n",
    "    for post in subreddit.hot(limit=post_limit):\n",
    "        # Print the title and score of each post\n",
    "        print(f\"  - [Score: {post.score}] {post.title}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"üö® Error fetching posts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071114a",
   "metadata": {},
   "source": [
    "# Store Detailled Post and comments from subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6b599",
   "metadata": {},
   "source": [
    "# Cleaning script for the comments. Removing links, emojis etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c3ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ The 'clean_text' function is now defined and ready to use.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"A function to clean raw text.\"\"\"\n",
    "    # Make text lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    # Remove special characters and numbers, keeping only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "print(\"‚úÖ The 'clean_text' function is now defined and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5b446",
   "metadata": {},
   "source": [
    "# Collect Targeted Feedback from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de05726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Searching r/n8n for top posts of all time containing feedback keywords...\n",
      "‚úÖ Successfully collected data for 249 posts.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# This is the targeted data collection script.\n",
    "# It creates the 'reddit_data' variable that the final AI script needs.\n",
    "\n",
    "reddit_data = []\n",
    "subreddit_name = \"n8n\"\n",
    "post_limit = 1000 # Using a large limit for a comprehensive search\n",
    "\n",
    "# The search query to find relevant feedback posts\n",
    "search_query = \"feedback OR idea OR suggestion OR bug OR error OR improve OR wish OR stuck OR help\"\n",
    "\n",
    "print(f\"üî¨ Searching r/{subreddit_name} for top posts of all time containing feedback keywords...\")\n",
    "\n",
    "try:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    # Use .search() to find the most relevant posts from all time\n",
    "    for post in subreddit.search(search_query, sort='top', time_filter='all', limit=post_limit):\n",
    "        post_data = {\n",
    "            \"id\": post.id,\n",
    "            \"title\": post.title,\n",
    "            \"score\": post.score,\n",
    "            \"url\": post.url,\n",
    "            \"body\": post.selftext,\n",
    "            \"comments\": []\n",
    "        }\n",
    "\n",
    "        # Fetch all comments, skipping the 'MoreComments' objects\n",
    "        for comment in post.comments.list():\n",
    "            if isinstance(comment, MoreComments):\n",
    "                continue\n",
    "            post_data[\"comments\"].append(comment.body)\n",
    "        \n",
    "        reddit_data.append(post_data)\n",
    "\n",
    "    print(f\"‚úÖ Successfully collected data for {len(reddit_data)} posts.\")\n",
    "    \n",
    "    if reddit_data:\n",
    "        # We don't need to print the example here, as the final report will show the output.\n",
    "        pass\n",
    "    else:\n",
    "        print(\"No posts found matching the search query.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"üö® Error collecting detailed data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1cf208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßº Applying cleaning function to all collected Reddit text...\n",
      "‚úÖ Cleaning complete for 249 posts.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"üßº Applying cleaning function to all collected Reddit text...\")\n",
    "\n",
    "# This is the crucial loop that adds the 'cleaned_' keys to your Reddit data\n",
    "for post in reddit_data:\n",
    "    post['cleaned_title'] = clean_text(post['title'])\n",
    "    post['cleaned_body'] = clean_text(post['body'])\n",
    "    post['cleaned_comments'] = [clean_text(comment) for comment in post['comments']]\n",
    "\n",
    "print(f\"‚úÖ Cleaning complete for {len(reddit_data)} posts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b7fa6",
   "metadata": {},
   "source": [
    "# Product Hunt Crawl - Important: Product Hunt is giving only a limit amount of reviews in our cas 42 out of 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf835b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Fetching all reviews for 'n8n-io' from Product Hunt...\n",
      "  - Fetched a page, total reviews collected: 20\n",
      "  - Fetched a page, total reviews collected: 40\n",
      "  - Fetched a page, total reviews collected: 42\n",
      "\n",
      "‚úÖ Finished! Successfully collected and cleaned 42 reviews.\n",
      "\n",
      "--- Example of Cleaned Reviews ---\n",
      "- i love your product\n",
      "- so excited to test the possibilities with notion automatizations\n",
      "- amazing solution\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "product_slug = \"n8n-io\"\n",
    "# We ask for 50 at a time, a common page size limit for APIs\n",
    "limit_per_page = 50\n",
    "\n",
    "# --- API and Header setup (same as before) ---\n",
    "api_url = \"https://api.producthunt.com/v2/api/graphql\"\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {ph_access_token}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# --- A more advanced query that includes pagination logic ---\n",
    "query = \"\"\"\n",
    "query getProductReviews($slug: String!, $limit: Int!, $after: String) {\n",
    "  post(slug: $slug) {\n",
    "    name\n",
    "    comments(first: $limit, after: $after) {\n",
    "      edges {\n",
    "        node {\n",
    "          body\n",
    "        }\n",
    "      }\n",
    "      pageInfo {\n",
    "        hasNextPage\n",
    "        endCursor\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# --- The Loop to Fetch All Pages ---\n",
    "product_hunt_reviews = []\n",
    "after_cursor = None # Start with no cursor to get the first page\n",
    "has_next_page = True\n",
    "\n",
    "print(f\"üìù Fetching all reviews for '{product_slug}' from Product Hunt...\")\n",
    "\n",
    "while has_next_page:\n",
    "    # Set the variables for the API request, including the cursor\n",
    "    request_body = {\n",
    "        \"query\": query,\n",
    "        \"variables\": {\"slug\": product_slug, \"limit\": limit_per_page, \"after\": after_cursor}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=request_body)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            ph_data = response.json()\n",
    "            post_info = ph_data['data']['post']\n",
    "            \n",
    "            if post_info:\n",
    "                comments_page = post_info['comments']\n",
    "                \n",
    "                # Clean and store the reviews from the current page\n",
    "                for comment in comments_page['edges']:\n",
    "                    cleaned_review = clean_text(comment['node']['body'])\n",
    "                    product_hunt_reviews.append(cleaned_review)\n",
    "                \n",
    "                # Check if there's a next page and get the new cursor\n",
    "                page_info = comments_page['pageInfo']\n",
    "                has_next_page = page_info['hasNextPage']\n",
    "                after_cursor = page_info['endCursor']\n",
    "                \n",
    "                print(f\"  - Fetched a page, total reviews collected: {len(product_hunt_reviews)}\")\n",
    "                \n",
    "            else:\n",
    "                # No post found for the slug\n",
    "                print(f\"üö® Could not find a product with slug '{product_slug}'.\")\n",
    "                break\n",
    "\n",
    "            # A small delay to be polite to the API\n",
    "            time.sleep(1)\n",
    "\n",
    "        else:\n",
    "            print(f\"üö® Error fetching from Product Hunt: Status code {response.status_code}\")\n",
    "            print(response.text)\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üö® An exception occurred: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úÖ Finished! Successfully collected and cleaned {len(product_hunt_reviews)} reviews.\")\n",
    "\n",
    "# --- Example of Cleaned Reviews ---\n",
    "print(\"\\n--- Example of Cleaned Reviews ---\")\n",
    "for review in product_hunt_reviews[:3]:\n",
    "    print(f\"- {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ce7f3",
   "metadata": {},
   "source": [
    "# Find the most common Phrases (N-Grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d29bd387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# installing the right PY package Natural Language Toolki (nltk)\n",
    "\n",
    "!/usr/local/bin/python3.11 -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df08e7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Downloading required NLTK data packages...\n",
      "‚úÖ NLTK data is ready.\n",
      "\n",
      "üî¨ Starting data analysis...\n",
      "‚úÖ Analysis Complete!\n",
      "üîé Top 10 Most Common Phrases (Trigrams):\n",
      "  - nn workflow: 134 times\n",
      "  - ai agent: 134 times\n",
      "  - thanks sharing: 112 times\n",
      "  - google sheets: 112 times\n",
      "  - using nn: 106 times\n",
      "  - let know: 104 times\n",
      "  - ai agents: 94 times\n",
      "  - dont know: 86 times\n",
      "  - nn workflows: 85 times\n",
      "  - code node: 85 times\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- FIX: Temporarily disable SSL verification ---\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# --- FIX: Directly download ALL necessary data packages ---\n",
    "print(\"‚öôÔ∏è Downloading required NLTK data packages...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"‚úÖ NLTK data is ready.\")\n",
    "\n",
    "\n",
    "# --- ANALYSIS ---\n",
    "print(\"\\nüî¨ Starting data analysis...\")\n",
    "\n",
    "# Combine all cleaned text into one big list\n",
    "all_text = []\n",
    "\n",
    "# Add cleaned Reddit data\n",
    "for post in reddit_data:\n",
    "    all_text.append(post['cleaned_title'])\n",
    "    all_text.append(post['cleaned_body'])\n",
    "    all_text.extend(post['cleaned_comments'])\n",
    "\n",
    "# Add cleaned Product Hunt reviews\n",
    "all_text.extend(product_hunt_reviews)\n",
    "\n",
    "# Join everything into a single block of text and tokenize into words\n",
    "full_text = ' '.join(all_text)\n",
    "words = nltk.word_tokenize(full_text)\n",
    "\n",
    "# --- Remove common \"stop words\" using NLTK's comprehensive list ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Add custom words to ignore based on previous output\n",
    "stop_words.update(['im', 'ive', 'would', 'also', 'like', 'get']) \n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# --- Generate and count three-word phrases (trigrams) ---\n",
    "# MODIFIED: Changed the number from 2 to 3\n",
    "trigrams = ngrams(filtered_words, 2)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "print(\"‚úÖ Analysis Complete!\")\n",
    "print(\"üîé Top 10 Most Common Phrases (Trigrams):\")\n",
    "for phrase, count in trigram_counts.most_common(10):\n",
    "    print(f\"  - {' '.join(phrase)}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586905a",
   "metadata": {},
   "source": [
    "# Using OpenAI to understand the user comments and potential feature requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236b623",
   "metadata": {},
   "source": [
    "# Install openAI library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26333ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.93.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.10.22)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/floriankrause/Library/Python/3.11/lib/python/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/bin/python3.11 -m pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2182b3",
   "metadata": {},
   "source": [
    "# Analyzing all Reddit comments and categorize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f8c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI Client configured successfully.\n",
      "\n",
      "üî¨ Analyzing all 9636 feedback items from all sources... (This will take a long time)\n",
      "\n",
      "‚úÖ AI analysis complete. Found 1099 relevant feedback items.\n",
      "\n",
      "üíæ Saving the comprehensive report to 'final_comprehensive_report.csv'...\n",
      "‚úÖ Report created successfully.\n",
      "\n",
      "üß† Generating AI summary for 'Low-Hanging Fruits (User Struggles)'...\n",
      "\n",
      "üß† Generating AI summary for 'Nice-to-Have (Feature Requests)'...\n",
      "\n",
      "üß† Generating AI summary for 'No Business Impact (Praise)'...\n",
      "\n",
      "‚úÖ Dashboard summaries generated and saved to 'dashboard_summaries.json'.\n",
      "\n",
      "--- AI-Generated Summaries Preview ---\n",
      "\n",
      "Low-Hanging Fruits:\n",
      "Users are encountering several common challenges with n8n, primarily centered around setup complexity, workflow management, and lack of clarity in documentation. Key areas for improvement include:\n",
      "\n",
      "1. **Enhanced Documentation**: Provide more detailed, beginner-friendly guides and video tutorials focused on common tasks, such as workflow creation, using key nodes (e.g., HTTP Request, Deep Research), and troubleshooting common issues.\n",
      "\n",
      "2. **User Interface Simplification**: Streamline the interface to reduce cognitive overload. Consider introducing guided workflows and visual aids that demystify the node setup process, allowing users to build more intuitive connections.\n",
      "\n",
      "3. **Error Handling and Debugging Tools**: Develop better tools for debugging workflows, including clear error messages and actionable resolutions to streamline troubleshooting.\n",
      "\n",
      "4. **Community Engagement and Support**: Establish an onboarding program or mentorship opportunities for new users to feel more supported as they learn.\n",
      "\n",
      "5. **Simplify Workflow Sharing**: Clarify the functionality and advantages of shared workflows, making access and usability more intuitive.\n",
      "\n",
      "6. **Feedback Mechanisms**: Implement mechanisms within the platform for users to easily report issues and suggest improvements, fostering an adaptive development process. \n",
      "\n",
      "These targeted improvements will enhance user experience and reduce the barriers to effective n8n usage.\n",
      "\n",
      "Nice-to-Have:\n",
      "User feedback indicates a demand for improvements in automation capabilities, integration options, and enhanced usability within n8n. Key themes include:\n",
      "\n",
      "1. **Integration Expansion**: Users request connections to platforms like LinkedIn, Pinterest, and Instagram, along with robust tools for scraping and managing diverse data sources. Implementing dedicated nodes for these platforms would simplify automating content and engagement.\n",
      "\n",
      "2. **Enhanced Usability**: Many users desire clearer documentation, streamlined onboarding, and tutorial resources. Providing walkthroughs, example workflows, and interactive help features would enhance user confidence and ease of use.\n",
      "\n",
      "3. **Workflow Management**: Users seek improved functionality for managing workflows, including better organization through directories, title and categorization enhancements, and advanced filtering options. These could include a centralized dashboard to visualize and manage workflows more effectively.\n",
      "\n",
      "4. **Automation & AI Features**: There is strong interest in AI capabilities, such as automated content generation and analysis. Integrating AI nodes for real-time suggestions and errors handling would cater to this demand.\n",
      "\n",
      "By prioritizing these enhancements, n8n can significantly elevate user experience and streamline automation processes for diverse workflows.\n",
      "\n",
      "No Business Impact:\n",
      "User feedback highlights n8n‚Äôs strength in task automation, flexibility, and ease of use, especially for non-coders. Key improvements include enhancing native Postgres integration for simpler interactions and addressing the ineffectiveness of the AI Agent node for better performance. Users value the workflow generator and appreciate time savings; however, frustrations with workflow re-runs and the need for idempotency must be addressed. Further, users are keen on clearer documentation around complex use cases and limitations to enhance usability. Emphasizing richer features for chatbot functionality and improving debugging capabilities would also elevate the user experience. Lastly, the suggestion to enable direct workflow sharing within n8n can facilitate better collaboration and learning among users. Implementing these improvements will not only solidify user satisfaction but also broaden n8n's appeal as a powerful, user-friendly automation tool.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# --- PART 1: CONFIGURE CLIENT AND PROMPT ---\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"üö® OpenAI API Key not found. Please add it to your .env file.\")\n",
    "else:\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    print(\"‚úÖ OpenAI Client configured successfully.\")\n",
    "\n",
    "# This prompt is for the first step: categorizing individual comments\n",
    "messages_for_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are a product analyst for a software company called **n8n**. Your task is to analyze user comments and extract actionable feedback **about the n8n product itself**.\n",
    "\n",
    "**CRITICAL RULE**: You must first determine if the comment is direct feedback about the n8n software. If the comment is about the user's own project performance (e.g., their newsletter's CTR), about the subreddit community, or is just a general polite comment (e.g., 'thank you'), classify it as **`Irrelevant`**.\n",
    "\n",
    "For relevant comments about the n8n product, classify them into one of these categories:\n",
    "- Struggle / Confusion\n",
    "- Bug Report\n",
    "- Feature Request\n",
    "- Positive Feedback\n",
    "- Negative Feedback\n",
    "\n",
    "Provide your output in a JSON format with two keys: `category` and `user_problem`. If there is no specific user problem, return `null` for that key.\"\"\"\n",
    "    },\n",
    "    { \"role\": \"user\", \"content\": \"**POST:** How I built this workflow.\\n**COMMENT:** This is great, but the n8n expression editor is confusing. I wish it had autocomplete.\"},\n",
    "    { \"role\": \"assistant\", \"content\": \"{\\n  \\\"category\\\": \\\"Negative Feedback\\\",\\n  \\\"user_problem\\\": \\\"The n8n expression editor is confusing and lacks an autocomplete feature, making it difficult to use.\\\"\\n}\"},\n",
    "    { \"role\": \"user\", \"content\": \"**POST:** Look at my new workflow!\\n**COMMENT:** This is awesome, great job!\"},\n",
    "    { \"role\": \"assistant\", \"content\": \"{\\n  \\\"category\\\": \\\"Irrelevant\\\",\\n  \\\"user_problem\\\": null\\n}\"}\n",
    "]\n",
    "\n",
    "def get_contextual_analysis(post, comment):\n",
    "    if isinstance(post, dict):\n",
    "        full_context = f\"**ORIGINAL POST TITLE:** {post['title']}\\n**ORIGINAL POST BODY:** {post['body'][:500]}...\\n**COMMENT:** {comment}\"\n",
    "    else:\n",
    "        full_context = comment\n",
    "    current_messages = messages_for_prompt + [{\"role\": \"user\", \"content\": full_context}]\n",
    "    try:\n",
    "        response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=current_messages, response_format={\"type\": \"json_object\"})\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# --- NEW: A more advanced function to generate dashboard summaries ---\n",
    "def generate_dashboard_summary(title, feedback_list):\n",
    "    print(f\"\\nüß† Generating AI summary for '{title}'...\")\n",
    "    if not feedback_list:\n",
    "        return f\"No data available for {title}.\"\n",
    "    \n",
    "    problems_str = \"\\n\".join([f\"- {item}\" for item in feedback_list])\n",
    "    \n",
    "    # This new prompt is much more specific and demanding\n",
    "    prompt = f\"\"\"You are a senior product manager at n8n. Your task is to write a concise, actionable summary (around 150 words) for a dashboard section.\n",
    "Based on the following list of user problems categorized under '{title}', synthesize the key themes and identify concrete improvements n8n can make.\n",
    "Do not start with a title. Begin the summary directly.\n",
    "\n",
    "**User Feedback:**\n",
    "{problems_str}\n",
    "\n",
    "**Actionable Summary for Dashboard:**\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {e}\"\n",
    "\n",
    "# --- PART 2: COMBINE ALL DATA & RUN FULL ANALYSIS ---\n",
    "\n",
    "final_results = []\n",
    "all_feedback_items = []\n",
    "\n",
    "# Gather Reddit comments with their post context\n",
    "if 'reddit_data' in locals():\n",
    "    for post in reddit_data:\n",
    "        for comment in post['comments']:\n",
    "            all_feedback_items.append({'comment': comment, 'context': post})\n",
    "\n",
    "# Add Product Hunt reviews\n",
    "if 'product_hunt_reviews' in locals():\n",
    "    for review in product_hunt_reviews:\n",
    "        all_feedback_items.append({'comment': review, 'context': 'Product Hunt Review'})\n",
    "\n",
    "# MODIFIED: Removed the sample_size limit to process all data\n",
    "print(f\"\\nüî¨ Analyzing all {len(all_feedback_items)} feedback items from all sources... (This will take a long time)\")\n",
    "\n",
    "for item in all_feedback_items:\n",
    "    comment_text = item['comment']\n",
    "    post_context = item['context']\n",
    "    \n",
    "    if len(comment_text) < 20 or \"[deleted]\" in comment_text or \"[removed]\" in comment_text:\n",
    "        continue\n",
    "    \n",
    "    analysis = get_contextual_analysis(post_context, comment_text)\n",
    "    \n",
    "    if analysis and analysis.get('category') and analysis['category'] != 'Irrelevant':\n",
    "        final_results.append({\n",
    "            \"source\": \"Reddit\" if isinstance(post_context, dict) else \"Product Hunt\",\n",
    "            \"comment\": comment_text,\n",
    "            \"category\": analysis['category'],\n",
    "            \"user_problem\": analysis.get('user_problem')\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ AI analysis complete. Found {len(final_results)} relevant feedback items.\")\n",
    "\n",
    "# --- PART 3: SAVE REPORT & GENERATE DASHBOARD SUMMARIES ---\n",
    "\n",
    "if final_results:\n",
    "    df = pd.DataFrame(final_results)\n",
    "    output_filename = 'final_comprehensive_report.csv'\n",
    "    print(f\"\\nüíæ Saving the comprehensive report to '{output_filename}'...\")\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ Report created successfully.\")\n",
    "\n",
    "    # Generate AI summaries for the dashboard\n",
    "    struggles = df[df['category'] == 'Struggle / Confusion']['user_problem'].dropna().tolist()\n",
    "    feature_requests = df[df['category'] == 'Feature Request']['user_problem'].dropna().tolist()\n",
    "    positive_feedback = df[df['category'] == 'Positive Feedback']['user_problem'].dropna().tolist()\n",
    "\n",
    "    low_hanging_summary = generate_dashboard_summary(\"Low-Hanging Fruits (User Struggles)\", struggles)\n",
    "    nice_to_have_summary = generate_dashboard_summary(\"Nice-to-Have (Feature Requests)\", feature_requests)\n",
    "    no_impact_summary = generate_dashboard_summary(\"No Business Impact (Praise)\", positive_feedback)\n",
    "    \n",
    "    dashboard_summaries = {\n",
    "        \"low_hanging_fruits\": low_hanging_summary,\n",
    "        \"nice_to_have\": nice_to_have_summary,\n",
    "        \"no_business_impact\": no_impact_summary\n",
    "    }\n",
    "    \n",
    "    with open('dashboard_summaries.json', 'w') as f:\n",
    "        json.dump(dashboard_summaries, f, indent=2)\n",
    "        \n",
    "    print(\"\\n‚úÖ Dashboard summaries generated and saved to 'dashboard_summaries.json'.\")\n",
    "    \n",
    "    print(\"\\n--- AI-Generated Summaries Preview ---\")\n",
    "    print(f\"\\nLow-Hanging Fruits:\\n{low_hanging_summary}\")\n",
    "    print(f\"\\nNice-to-Have:\\n{nice_to_have_summary}\")\n",
    "    print(f\"\\nNo Business Impact:\\n{no_impact_summary}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo relevant comments were found to generate a report.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
