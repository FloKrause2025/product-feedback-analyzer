{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ebd48d7",
   "metadata": {},
   "source": [
    "## Product Analyzer Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe97bb",
   "metadata": {},
   "source": [
    "# Section 1: Setup & Environment\n",
    "\n",
    "In this section, we will install the necessary libraries and set up our credentials to connect to the Reddit and Product Hunt APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9af241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (7.8.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.32.4)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2025.7.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/bin/python3.11 -m pip install praw requests python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855cf300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Reddit credentials\n",
    "reddit_client_id = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "reddit_client_secret = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "\n",
    "# Get the Product Hunt credentials\n",
    "ph_api_key = os.getenv(\"PRODUCT_HUNT_API_KEY\")\n",
    "ph_access_token = os.getenv(\"PRODUCT_HUNT_ACCESS_TOKEN\")\n",
    "\n",
    "# Check if the keys were loaded correctly\n",
    "if reddit_client_id and ph_api_key:\n",
    "    print(\"‚úÖ API keys loaded successfully!\")\n",
    "else:\n",
    "    print(\"üö® Error: Could not load API keys. Check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36a52c",
   "metadata": {},
   "source": [
    "# Data collection Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef87abf",
   "metadata": {},
   "source": [
    "# Connection to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df2c16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Reddit as: None (Read-Only Mode: True)\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "# Use the loaded keys to initialize the Reddit API connection\n",
    "# A user_agent is a unique name for your script so Reddit knows who is making requests.\n",
    "try:\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=reddit_client_id,\n",
    "        client_secret=reddit_client_secret,\n",
    "        user_agent=\"Feature-Analyzer by Flo\"\n",
    "    )\n",
    "\n",
    "    # Check the connection status\n",
    "    print(f\"‚úÖ Connected to Reddit as: {reddit.user.me()} (Read-Only Mode: {reddit.read_only})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"üö® Error connecting to Reddit: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c8374",
   "metadata": {},
   "source": [
    "# Fetching Data from your Subreddit - in our case \"n8n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5110c909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Fetching the top 10 hot posts from r/n8n...\n",
      "  - [Score: 2] Weekly Self Promotion Thread\n",
      "  - [Score: 49] Anyone here looking for a job?\n",
      "  - [Score: 25] GitHub Releases to Marketing\n",
      "  - [Score: 5] Hiring n8n Expert (Bonus if Full Stack w/ Node.js + React)\n",
      "  - [Score: 3] How I Use Redis to Cache Google API Data in n8n (and Why You Should Too)\n",
      "  - [Score: 5] Open-Source React AI Chat Widget for n8n\n",
      "  - [Score: 10] Add Auto-Suggestion Replies to Your n8n Chatbots\n",
      "  - [Score: 61] Pain Point Scraper\n",
      "  - [Score: 3] FreelancerBot: AI-Powered Automated Bidding System with RAG & Multi-Agent Pipeline\n",
      "  - [Score: 2] SaaS with N8N back-end - license question\n"
     ]
    }
   ],
   "source": [
    "# Define the target subreddit and how many posts to fetch\n",
    "subreddit_name = \"n8n\"\n",
    "post_limit = 10\n",
    "\n",
    "print(f\"üî• Fetching the top {post_limit} hot posts from r/{subreddit_name}...\")\n",
    "\n",
    "try:\n",
    "    # Point our connection to the target subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Fetch the 'hot' posts and loop through them\n",
    "    for post in subreddit.hot(limit=post_limit):\n",
    "        # Print the title and score of each post\n",
    "        print(f\"  - [Score: {post.score}] {post.title}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"üö® Error fetching posts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071114a",
   "metadata": {},
   "source": [
    "# Store Detailled Post and comments from subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6b599",
   "metadata": {},
   "source": [
    "# Cleaning script for the comments. Removing links, emojis etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c3ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ The 'clean_text' function is now defined and ready to use.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"A function to clean raw text.\"\"\"\n",
    "    # Make text lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    # Remove special characters and numbers, keeping only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "print(\"‚úÖ The 'clean_text' function is now defined and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5b446",
   "metadata": {},
   "source": [
    "# Collect Targeted Feedback from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de05726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Searching r/n8n for top posts of all time containing feedback keywords...\n",
      "‚úÖ Successfully collected data for 249 posts.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# This is the targeted data collection script.\n",
    "# It creates the 'reddit_data' variable that the final AI script needs.\n",
    "\n",
    "reddit_data = []\n",
    "subreddit_name = \"n8n\"\n",
    "post_limit = 1000 # Using a large limit for a comprehensive search\n",
    "\n",
    "# The search query to find relevant feedback posts\n",
    "search_query = \"feedback OR idea OR suggestion OR bug OR error OR improve OR wish OR stuck OR help\"\n",
    "\n",
    "print(f\"üî¨ Searching r/{subreddit_name} for top posts of all time containing feedback keywords...\")\n",
    "\n",
    "try:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    # Use .search() to find the most relevant posts from all time\n",
    "    for post in subreddit.search(search_query, sort='top', time_filter='all', limit=post_limit):\n",
    "        post_data = {\n",
    "            \"id\": post.id,\n",
    "            \"title\": post.title,\n",
    "            \"score\": post.score,\n",
    "            \"url\": post.url,\n",
    "            \"body\": post.selftext,\n",
    "            \"comments\": []\n",
    "        }\n",
    "\n",
    "        # Fetch all comments, skipping the 'MoreComments' objects\n",
    "        for comment in post.comments.list():\n",
    "            if isinstance(comment, MoreComments):\n",
    "                continue\n",
    "            post_data[\"comments\"].append(comment.body)\n",
    "        \n",
    "        reddit_data.append(post_data)\n",
    "\n",
    "    print(f\"‚úÖ Successfully collected data for {len(reddit_data)} posts.\")\n",
    "    \n",
    "    if reddit_data:\n",
    "        # We don't need to print the example here, as the final report will show the output.\n",
    "        pass\n",
    "    else:\n",
    "        print(\"No posts found matching the search query.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"üö® Error collecting detailed data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1cf208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßº Applying cleaning function to all collected Reddit text...\n",
      "‚úÖ Cleaning complete for 249 posts.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"üßº Applying cleaning function to all collected Reddit text...\")\n",
    "\n",
    "# This is the crucial loop that adds the 'cleaned_' keys to your Reddit data\n",
    "for post in reddit_data:\n",
    "    post['cleaned_title'] = clean_text(post['title'])\n",
    "    post['cleaned_body'] = clean_text(post['body'])\n",
    "    post['cleaned_comments'] = [clean_text(comment) for comment in post['comments']]\n",
    "\n",
    "print(f\"‚úÖ Cleaning complete for {len(reddit_data)} posts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b7fa6",
   "metadata": {},
   "source": [
    "# Product Hunt Crawl - Important: Product Hunt is giving only a limit amount of reviews in our cas 42 out of 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf835b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Fetching all reviews for 'n8n-io' from Product Hunt...\n",
      "  - Fetched a page, total reviews collected: 20\n",
      "  - Fetched a page, total reviews collected: 40\n",
      "  - Fetched a page, total reviews collected: 42\n",
      "\n",
      "‚úÖ Finished! Successfully collected and cleaned 42 reviews.\n",
      "\n",
      "--- Example of Cleaned Reviews ---\n",
      "- i love your product\n",
      "- so excited to test the possibilities with notion automatizations\n",
      "- amazing solution\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "product_slug = \"n8n-io\"\n",
    "# We ask for 50 at a time, a common page size limit for APIs\n",
    "limit_per_page = 50\n",
    "\n",
    "# --- API and Header setup (same as before) ---\n",
    "api_url = \"https://api.producthunt.com/v2/api/graphql\"\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {ph_access_token}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# --- A more advanced query that includes pagination logic ---\n",
    "query = \"\"\"\n",
    "query getProductReviews($slug: String!, $limit: Int!, $after: String) {\n",
    "  post(slug: $slug) {\n",
    "    name\n",
    "    comments(first: $limit, after: $after) {\n",
    "      edges {\n",
    "        node {\n",
    "          body\n",
    "        }\n",
    "      }\n",
    "      pageInfo {\n",
    "        hasNextPage\n",
    "        endCursor\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# --- The Loop to Fetch All Pages ---\n",
    "product_hunt_reviews = []\n",
    "after_cursor = None # Start with no cursor to get the first page\n",
    "has_next_page = True\n",
    "\n",
    "print(f\"üìù Fetching all reviews for '{product_slug}' from Product Hunt...\")\n",
    "\n",
    "while has_next_page:\n",
    "    # Set the variables for the API request, including the cursor\n",
    "    request_body = {\n",
    "        \"query\": query,\n",
    "        \"variables\": {\"slug\": product_slug, \"limit\": limit_per_page, \"after\": after_cursor}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=request_body)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            ph_data = response.json()\n",
    "            post_info = ph_data['data']['post']\n",
    "            \n",
    "            if post_info:\n",
    "                comments_page = post_info['comments']\n",
    "                \n",
    "                # Clean and store the reviews from the current page\n",
    "                for comment in comments_page['edges']:\n",
    "                    cleaned_review = clean_text(comment['node']['body'])\n",
    "                    product_hunt_reviews.append(cleaned_review)\n",
    "                \n",
    "                # Check if there's a next page and get the new cursor\n",
    "                page_info = comments_page['pageInfo']\n",
    "                has_next_page = page_info['hasNextPage']\n",
    "                after_cursor = page_info['endCursor']\n",
    "                \n",
    "                print(f\"  - Fetched a page, total reviews collected: {len(product_hunt_reviews)}\")\n",
    "                \n",
    "            else:\n",
    "                # No post found for the slug\n",
    "                print(f\"üö® Could not find a product with slug '{product_slug}'.\")\n",
    "                break\n",
    "\n",
    "            # A small delay to be polite to the API\n",
    "            time.sleep(1)\n",
    "\n",
    "        else:\n",
    "            print(f\"üö® Error fetching from Product Hunt: Status code {response.status_code}\")\n",
    "            print(response.text)\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üö® An exception occurred: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úÖ Finished! Successfully collected and cleaned {len(product_hunt_reviews)} reviews.\")\n",
    "\n",
    "# --- Example of Cleaned Reviews ---\n",
    "print(\"\\n--- Example of Cleaned Reviews ---\")\n",
    "for review in product_hunt_reviews[:3]:\n",
    "    print(f\"- {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ce7f3",
   "metadata": {},
   "source": [
    "# Find the most common Phrases (N-Grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d29bd387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# installing the right PY package Natural Language Toolki (nltk)\n",
    "\n",
    "!/usr/local/bin/python3.11 -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df08e7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Downloading required NLTK data packages...\n",
      "‚úÖ NLTK data is ready.\n",
      "\n",
      "üî¨ Starting data analysis...\n",
      "‚úÖ Analysis Complete!\n",
      "üîé Top 10 Most Common Phrases (Trigrams):\n",
      "  - nn workflow: 134 times\n",
      "  - ai agent: 134 times\n",
      "  - thanks sharing: 112 times\n",
      "  - google sheets: 112 times\n",
      "  - using nn: 106 times\n",
      "  - let know: 104 times\n",
      "  - ai agents: 94 times\n",
      "  - dont know: 86 times\n",
      "  - nn workflows: 85 times\n",
      "  - code node: 85 times\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- FIX: Temporarily disable SSL verification ---\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# --- FIX: Directly download ALL necessary data packages ---\n",
    "print(\"‚öôÔ∏è Downloading required NLTK data packages...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"‚úÖ NLTK data is ready.\")\n",
    "\n",
    "\n",
    "# --- ANALYSIS ---\n",
    "print(\"\\nüî¨ Starting data analysis...\")\n",
    "\n",
    "# Combine all cleaned text into one big list\n",
    "all_text = []\n",
    "\n",
    "# Add cleaned Reddit data\n",
    "for post in reddit_data:\n",
    "    all_text.append(post['cleaned_title'])\n",
    "    all_text.append(post['cleaned_body'])\n",
    "    all_text.extend(post['cleaned_comments'])\n",
    "\n",
    "# Add cleaned Product Hunt reviews\n",
    "all_text.extend(product_hunt_reviews)\n",
    "\n",
    "# Join everything into a single block of text and tokenize into words\n",
    "full_text = ' '.join(all_text)\n",
    "words = nltk.word_tokenize(full_text)\n",
    "\n",
    "# --- Remove common \"stop words\" using NLTK's comprehensive list ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Add custom words to ignore based on previous output\n",
    "stop_words.update(['im', 'ive', 'would', 'also', 'like', 'get']) \n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# --- Generate and count three-word phrases (trigrams) ---\n",
    "# MODIFIED: Changed the number from 2 to 3\n",
    "trigrams = ngrams(filtered_words, 2)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "print(\"‚úÖ Analysis Complete!\")\n",
    "print(\"üîé Top 10 Most Common Phrases (Trigrams):\")\n",
    "for phrase, count in trigram_counts.most_common(10):\n",
    "    print(f\"  - {' '.join(phrase)}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586905a",
   "metadata": {},
   "source": [
    "# Using OpenAI to understand the user comments and potential feature requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236b623",
   "metadata": {},
   "source": [
    "# Install openAI library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26333ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.93.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.10.22)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/floriankrause/Library/Python/3.11/lib/python/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/bin/python3.11 -m pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2182b3",
   "metadata": {},
   "source": [
    "# Analyzing all Reddit comments and categorize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI Client configured successfully.\n",
      "\n",
      "üî¨ Analyzing a sample of 600 out of 9628 feedback items...\n",
      "\n",
      "‚úÖ AI categorization complete. Found 30 relevant feedback items.\n",
      "\n",
      "üíæ Saving the comprehensive report to 'final_comprehensive_report.csv'...\n",
      "‚úÖ Report created successfully.\n",
      "\n",
      "üß† Generating AI summary for 'Low-Hanging Fruits (Top Struggles)'...\n",
      "\n",
      "üß† Generating AI summary for 'Nice-to-Have (Top Feature Requests)'...\n",
      "\n",
      "üß† Generating AI summary for 'No Business Impact (Recent Praise)'...\n",
      "\n",
      "‚úÖ Dashboard summaries generated and saved to 'dashboard_summaries.json'.\n",
      "\n",
      "--- AI-Generated Summaries ---\n",
      "\n",
      "Low-Hanging Fruits:\n",
      "**Low-Hanging Fruits (Top Struggles):** Users are primarily grappling with the quality and reliability of the scraped content processed by the LLM, leading to prior issues. Additionally, many express frustration with the limitations of n8n, particularly in creating advanced automations and the complexities of workflow maintenance. Troubleshooting inconsistencies hinder efficient client handoffs, while users seek clearer guidance on utilizing the HTTP Request node and understanding the value of downloaded workflows versus those from the n8n website. Overall, there is a notable confusion surrounding workflow interactions and registration requirements, contributing to users feeling overwhelmed by the platform's capabilities.\n",
      "\n",
      "Nice-to-Have:\n",
      "**Nice-to-Have (Top Feature Requests):** Users have expressed a strong desire for enhanced content integration and automation capabilities within n8n. Key requests include the ability to pull in LinkedIn content, automate postings to Instagram and Reddit, and implement features for flagging spam and categorizing workflows. There is also a significant demand for mobile support, including a dedicated app with push notifications and topic selection. Furthermore, users are seeking better guidance on creating and managing RSS feeds, alongside structured tools like a searchable directory, README files, and AI assistance to improve workflow file management and usability. Overall, these enhancements reflect a need for streamlined automation, improved content accessibility, and better organizational features within the platform.\n",
      "\n",
      "No Business Impact:\n",
      "**No Business Impact (Recent Praise)**: Users have expressed appreciation for the flexibility and improved functionality offered by the HTTP Request node, which has significantly enhanced their workflow-building experience. Additionally, positive feedback highlights the transition to n8n as a pivotal move for better data control, version control, and debugging capabilities. Suggestions for printable resources and learning opportunities, such as basic JavaScript, indicate a desire for more accessible tools to enhance user understanding and productivity, though these do not directly impact business metrics.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# --- PART 1: CONFIGURE CLIENT AND PROMPT ---\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"üö® OpenAI API Key not found. Please add it to your .env file.\")\n",
    "else:\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    print(\"‚úÖ OpenAI Client configured successfully.\")\n",
    "\n",
    "# This prompt is for the first step: categorizing individual comments\n",
    "messages_for_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are a product analyst for a software company called **n8n**. Your task is to analyze user comments and extract actionable feedback **about the n8n product itself**.\n",
    "\n",
    "**CRITICAL RULE**: You must first determine if the comment is direct feedback about the n8n software. If the comment is about the user's own project performance (e.g., their newsletter's CTR), about the subreddit community, or is just a general polite comment (e.g., 'thank you'), classify it as **`Irrelevant`**.\n",
    "\n",
    "For relevant comments about the n8n product, classify them into one of these categories:\n",
    "- Struggle / Confusion\n",
    "- Bug Report\n",
    "- Feature Request\n",
    "- Positive Feedback\n",
    "- Negative Feedback\n",
    "\n",
    "Provide your output in a JSON format with two keys: `category` and `user_problem`. If there is no specific user problem, return `null` for that key.\"\"\"\n",
    "    },\n",
    "    { \"role\": \"user\", \"content\": \"**POST:** How I built this workflow.\\n**COMMENT:** This is great, but the n8n expression editor is confusing. I wish it had autocomplete.\"},\n",
    "    { \"role\": \"assistant\", \"content\": \"{\\n  \\\"category\\\": \\\"Negative Feedback\\\",\\n  \\\"user_problem\\\": \\\"The n8n expression editor is confusing and lacks an autocomplete feature, making it difficult to use.\\\"\\n}\"},\n",
    "    { \"role\": \"user\", \"content\": \"**POST:** Look at my new workflow!\\n**COMMENT:** This is awesome, great job!\"},\n",
    "    { \"role\": \"assistant\", \"content\": \"{\\n  \\\"category\\\": \\\"Irrelevant\\\",\\n  \\\"user_problem\\\": null\\n}\"}\n",
    "]\n",
    "\n",
    "def get_contextual_analysis(post, comment):\n",
    "    if isinstance(post, dict):\n",
    "        full_context = f\"**ORIGINAL POST TITLE:** {post['title']}\\n**ORIGINAL POST BODY:** {post['body'][:500]}...\\n**COMMENT:** {comment}\"\n",
    "    else:\n",
    "        full_context = comment\n",
    "    current_messages = messages_for_prompt + [{\"role\": \"user\", \"content\": full_context}]\n",
    "    try:\n",
    "        response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=current_messages, response_format={\"type\": \"json_object\"})\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# --- NEW: A more advanced function to generate dashboard summaries ---\n",
    "def generate_dashboard_summary(title, feedback_list):\n",
    "    print(f\"\\nüß† Generating AI summary for '{title}'...\")\n",
    "    if not feedback_list:\n",
    "        return f\"No data available for {title}.\"\n",
    "    \n",
    "    problems_str = \"\\n\".join([f\"- {item}\" for item in feedback_list])\n",
    "    \n",
    "    # This new prompt is much more specific and demanding\n",
    "    prompt = f\"\"\"You are a senior product manager at n8n. Your task is to write a concise, actionable summary (around 150 words) for a dashboard section.\n",
    "Based on the following list of user problems categorized under '{title}', synthesize the key themes and identify concrete improvements n8n can make.\n",
    "Do not start with a title. Begin the summary directly.\n",
    "\n",
    "**User Feedback:**\n",
    "{problems_str}\n",
    "\n",
    "**Actionable Summary for Dashboard:**\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {e}\"\n",
    "\n",
    "# --- PART 2: COMBINE ALL DATA & RUN ANALYSIS ON A SAMPLE ---\n",
    "\n",
    "final_results = []\n",
    "all_feedback_items = []\n",
    "\n",
    "if 'reddit_data' in locals():\n",
    "    for post in reddit_data:\n",
    "        for comment in post['comments']:\n",
    "            all_feedback_items.append({'comment': comment, 'context': post})\n",
    "\n",
    "if 'product_hunt_reviews' in locals():\n",
    "    for review in product_hunt_reviews:\n",
    "        all_feedback_items.append({'comment': review, 'context': 'Product Hunt Review'})\n",
    "\n",
    "# MODIFIED: Added a sample_size limit for testing\n",
    "sample_size = 2000\n",
    "print(f\"\\nüî¨ Analyzing a sample of {sample_size} out of {len(all_feedback_items)} feedback items... (This can be changed by editing 'sample_size')\")\n",
    "\n",
    "for item in all_feedback_items[:sample_size]:\n",
    "    comment_text = item['comment']\n",
    "    post_context = item['context']\n",
    "    \n",
    "    if len(comment_text) < 20 or \"[deleted]\" in comment_text or \"[removed]\" in comment_text:\n",
    "        continue\n",
    "    \n",
    "    analysis = get_contextual_analysis(post_context, comment_text)\n",
    "    \n",
    "    if analysis and analysis.get('category') and analysis['category'] != 'Irrelevant':\n",
    "        final_results.append({\n",
    "            \"source\": \"Reddit\" if isinstance(post_context, dict) else \"Product Hunt\",\n",
    "            \"comment\": comment_text,\n",
    "            \"category\": analysis['category'],\n",
    "            \"user_problem\": analysis.get('user_problem')\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ AI analysis complete. Found {len(final_results)} relevant feedback items.\")\n",
    "\n",
    "# --- PART 3: SAVE REPORT & GENERATE DASHBOARD SUMMARIES ---\n",
    "\n",
    "if final_results:\n",
    "    df = pd.DataFrame(final_results)\n",
    "    output_filename = 'final_comprehensive_report.csv'\n",
    "    print(f\"\\nüíæ Saving the comprehensive report to '{output_filename}'...\")\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ Report created successfully.\")\n",
    "\n",
    "    # Generate AI summaries for the dashboard\n",
    "    struggles = df[df['category'] == 'Struggle / Confusion']['user_problem'].dropna().tolist()\n",
    "    feature_requests = df[df['category'] == 'Feature Request']['user_problem'].dropna().tolist()\n",
    "    positive_feedback = df[df['category'] == 'Positive Feedback']['user_problem'].dropna().tolist()\n",
    "\n",
    "    low_hanging_summary = generate_dashboard_summary(\"Low-Hanging Fruits (User Struggles)\", struggles)\n",
    "    nice_to_have_summary = generate_dashboard_summary(\"Nice-to-Have (Feature Requests)\", feature_requests)\n",
    "    no_impact_summary = generate_dashboard_summary(\"No Business Impact (Praise)\", positive_feedback)\n",
    "    \n",
    "    dashboard_summaries = {\n",
    "        \"low_hanging_fruits\": low_hanging_summary,\n",
    "        \"nice_to_have\": nice_to_have_summary,\n",
    "        \"no_business_impact\": no_impact_summary\n",
    "    }\n",
    "    \n",
    "    with open('dashboard_summaries.json', 'w') as f:\n",
    "        json.dump(dashboard_summaries, f, indent=2)\n",
    "        \n",
    "    print(\"\\n‚úÖ Dashboard summaries generated and saved to 'dashboard_summaries.json'.\")\n",
    "    \n",
    "    print(\"\\n--- AI-Generated Summaries Preview ---\")\n",
    "    print(f\"\\nLow-Hanging Fruits:\\n{low_hanging_summary}\")\n",
    "    print(f\"\\nNice-to-Have:\\n{nice_to_have_summary}\")\n",
    "    print(f\"\\nNo Business Impact:\\n{no_impact_summary}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo relevant comments were found to generate a report.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
